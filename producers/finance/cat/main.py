import argparse
import asyncio
from datetime import datetime, timedelta
import logging
import pandas as pd
from typing import Optional, Required, List, Union, Any, Dict

from spotlight_utils.main import generate_datestrings, create_table_from_df, load_chunk_to_clickhouse, fetch_with_adaptive_concurrency
from spotlight_utils.config import ch_settings


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



def parse_args():
    parser = argparse.ArgumentParser(description="Fetch and process tables from CAT PDF reports, then load them into a Clickhouse database.")
    parser.add_argument('--start_date', type=str, default='20240101', help='Start date in YYYYMMDD format')
    parser.add_argument('--end_date', type=str, default='today', help='End date in YYYYMMDD format (or "today")')

    return parser.parse_args() 



async def main(start_date, end_date):
    
    """Fetch PDF from URLs generated by date range and extract text from the last 8 pages as a list of strings."""
    
    print(f"Pulling data for date range: {start_date}-{end_date}")

    datestrings = generate_datestrings(start_date=start_date, end_date=end_date)

    urls = [f'https://www.catnmsplan.com/sites/default/files/{datetime.strptime(datestring, "%Y%m%d").strftime("%Y-%m")}/{datetime.strptime(datestring, "%Y%m%d").strftime("%m.%d.%y")}-Monthly-CAT-Update.pdf'
        for datestring in datestrings]

    await fetch_with_adaptive_concurrency(
        urls=urls,
        chunk_size=100000,
        transform_func=transform_data
    )




async def transform_data(data, url) -> None:
    ''' Takes in raw response from concurrent fetch processor and inserts cleaned dataframes into sorted tables '''
    print("Processing pdf")
    params = gen_params(data)
    for report in params:
        combined_df = pd.DataFrame()
        for trade_type in params[report]['pages']:
            print(f"\nProcessing: {report}-{trade_type}")
            # Pull for specific trade type (Options or Equities)
            rawdata = data[params[report]['pages'][trade_type]]
            # Get data into formatted list
            try:
                combined = [row for sublist in rawdata for row in sublist[1:]]
            except:
                combined = rawdata
            print(f"Converting {report} report to df...")
            df = list_to_df(data=combined, params=params[report])
            logger.info("Generating unique IDs...")
            df['ID'] = 'cat' + report.lower()[:3] + df['Date'].dt.strftime('%Y%m%d').replace('-', '') + trade_type[:2]
            df['Source_URL'] = url
            # df['Data Provider'] = 'FINRA'     # Currently this is the only provider so it's implied
            df['Trade_Type'] = trade_type.capitalize()  # Add the trade type column
            print(f'\nCombiniing dfs: {df.head} & {combined_df}') 
            combined_df = pd.concat([combined_df, df])

        # Determine the table name based on the report type
        rptname=report.capitalize().replace(' ', '_')
        table_name = f'CAT_{rptname}_dev'

        print(f"Loading {rptname} into {table_name}...")
        await create_table_from_df(df=combined_df, table_name=table_name, key_col='ID', ch_settings=ch_settings)
        await load_chunk_to_clickhouse(df=combined_df, table_name=table_name, ch_settings=ch_settings)
        logger.info("...Success")




def list_to_df(data: List[List[Any]], params: Dict[str, Any]) -> pd.DataFrame:
    ''' Converts arrays output by pdfplumber to cleaned dataframe '''
    
    columns = list(params['schema'].keys())
    
    logger.info("Padding columns: ", columns)
    padded = [row[:len(columns)] + [None] * (len(columns) - len(row[:len(columns)])) for row in data]
    df = pd.DataFrame(padded, columns=columns)
    
    logger.info("Cleaning df: ", df.head(3))
    
    df = df[~df['Date'].str.contains('Trade|Date', na=False)].copy()     # Removes rows with non-date values in 'Date' column
    
    try:   # Convert date
        df['Date'] = pd.to_datetime(df['Date'], format=params['date format'])
    except ValueError as e:
        print(f"Error parsing date: {e}")
    # Remove special characters
    df = df.replace(' ', '', regex=True).replace('%', '', regex=True).replace(',', '', regex=True).reset_index(drop=True)
    
    for column, dtype in params['schema'].items():
        print(column, dtype)
        if column in df.columns:

            try:    # assert schema to df from config dict
                df[column] = df[column].astype(dtype)
            except Exception as e:
                print(f'Problem setting type for column {column}')
                print(e)
    return df



def gen_params(data):
    ''' Infer location in pdf of proper tables from features in pdf '''

    dynamic_slices = generate_slices(len(data)) # Slices determine what page each report is on (sometimes they take up 4 pages, sometimes 8, but the distribution of their allotted space remains the same)

    params = { 
        'rolling': {
            'schema': {
                'Date': 'datetime64[ns]',
                'Late': 'float',
                'Rejection_Initial': 'float',
                'Rejection_Adjusted': 'float',
                'Intrafirm_Initial': 'float',
                'Intrafirm_Adjusted': 'float',
                'Interfirm_Sent_Initial': 'float',
                'Interfirm_Sent_Adjusted': 'float',
                'Interfirm_Received_Initial': 'float',
                'Interfirm_Received_Adjusted': 'float',
                'Exchange_Initial': 'float',
                'Exchange_Adjusted': 'float',
                'Trade_Initial': 'float',
                'Trade_Adjusted': 'float',
                'Overall_Error_Rate_Initial': 'float',
                'Overall_Error_Rate_Adjusted': 'float',
                'Trade_Type': 'str',
                'Source_URL': 'str',
                'ID': 'str',
            },
            'date format': '%m/%d/%Y',
            'pages': dynamic_slices['rolling']
        },
        'trade stats': {
            'schema': {
                'Date': 'datetime64[ns]',
                'Processed': 'int',
                'Accepted': 'int',
                'Late': 'int',
                'Overall_Errors_Count': 'int',
                'Trade_Type': 'str',
                'Source_URL': 'str',
                'ID': 'str'
            },
            'date format': '%Y-%m-%d',
            'pages': dynamic_slices['trade stats']
        }
    }

    return params




def generate_slices(total_pages):
    """
    Generate dynamic slices for 'pages' based on total number of pages and page size.

    Args:
    - total_pages (int): Total number of pages available.
    - page_size (int): Size of each page group.

    Returns:
    - dict: A dictionary with dynamic slices for 'equities' and 'options'.
    """
    idx = total_pages // 4

    # Generate slices based on the number of groups
    slices = {
        'rolling': {
            'equities': slice(0, min(idx, total_pages)),
            'options': slice(idx, min(idx * 2, total_pages))
        },
        'trade stats': {
            'equities': slice(2 * idx, 3 * idx),
            'options': slice(3 * idx, 4 * idx),
        }
    }
    return slices




if __name__ == "__main__":
    args = parse_args()
    asyncio.run(main(start_date=args.start_date, end_date=args.end_date))